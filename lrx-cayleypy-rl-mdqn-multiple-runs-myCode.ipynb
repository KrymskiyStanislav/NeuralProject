import torch
import torch.nn as nn
import torch.nn.functional as F

# Basic config
n = 4  # permutation size
identity = torch.arange(n)
generators = [
    torch.tensor([1, 0, 2, 3]),  # swap 0 and 1
    torch.tensor([0, 2, 1, 3]),  # swap 1 and 2
    torch.tensor([0, 1, 3, 2])   # swap 2 and 3
]
num_generators = len(generators)

# Model
class PermModel(nn.Module):
    def __init__(self, n, emb_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(n, emb_dim)
        self.encoder = nn.Sequential(
            nn.Linear(n * emb_dim, hidden_dim),
            nn.ReLU()
        )
        self.token_head = nn.Linear(hidden_dim, num_generators + 1)  # +1 for stop token 'S'
        self.length_head = nn.Linear(hidden_dim, 1)

    def forward(self, perm):  # perm: (B, n)
        x = self.embedding(perm)  # (B, n, emb_dim)
        x = x.view(x.size(0), -1)  # flatten
        h = self.encoder(x)        # (B, hidden_dim)
        token_logits = self.token_head(h)
        length_est = self.length_head(h)
        return token_logits, length_est.squeeze(1)

# Sample run
model = PermModel(n=4, emb_dim=8, hidden_dim=32)
sample_perm = torch.tensor([[2, 0, 1, 3]])  # (B=1, n=4)
logits, length = model(sample_perm)

print("Token logits (generator or 'S'):", logits)
print("Estimated distance to identity:", length.item()) 
